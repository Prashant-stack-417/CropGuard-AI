{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üå± CropGuard AI ‚Äî Model Training (PyTorch)\n",
                "\n",
                "**Dataset:** [20K+ Multi-Class Crop Disease Images](https://www.kaggle.com/datasets/jawadali1045/20k-multi-class-crop-disease-images)  \n",
                "**Path:** `U:\\dataset` (42 classes, pre-split Train/Validation)  \n",
                "**Model:** MobileNetV2 (Transfer Learning)  \n",
                "**Runtime:** Python 3.14 + PyTorch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Imports & Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import copy\n",
                "import time\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "from torchvision import datasets, transforms, models\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "USE_CUDA = device.type == 'cuda'\n",
                "NON_BLOCKING = USE_CUDA\n",
                "\n",
                "print(f'PyTorch: {torch.__version__}')\n",
                "print(f'Device:  {device}')\n",
                "if USE_CUDA:\n",
                "    print(f'GPU:     {torch.cuda.get_device_name(0)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Dataset Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚îÄ‚îÄ Dataset paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "DATASET_ROOT = Path(r'U:\\dataset')\n",
                "TRAIN_DIR = DATASET_ROOT / 'Train'\n",
                "VAL_DIR   = DATASET_ROOT / 'Validation'\n",
                "\n",
                "print(f'Train dir: {TRAIN_DIR}  (exists: {TRAIN_DIR.exists()})')\n",
                "print(f'Val dir:   {VAL_DIR}  (exists: {VAL_DIR.exists()})')\n",
                "\n",
                "# List all classes\n",
                "classes = sorted([d.name for d in TRAIN_DIR.iterdir() if d.is_dir()])\n",
                "NUM_CLASSES = len(classes)\n",
                "print(f'\\nFound {NUM_CLASSES} classes:')\n",
                "for i, c in enumerate(classes):\n",
                "    train_count = len(list((TRAIN_DIR / c).glob('*')))\n",
                "    val_count = len(list((VAL_DIR / c).glob('*'))) if (VAL_DIR / c).exists() else 0\n",
                "    print(f'  {i:2d}. {c:<40s} ‚Äî Train: {train_count:4d}  Val: {val_count:4d}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "IMG_SIZE = 224\n",
                "BATCH_SIZE = 32\n",
                "NUM_WORKERS = 0  # Windows: keep at 0\n",
                "PIN_MEMORY = USE_CUDA\n",
                "SEED = 42\n",
                "\n",
                "torch.manual_seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "\n",
                "# ‚îÄ‚îÄ Transforms (ImageNet normalisation) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
                "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
                "\n",
                "train_transform = transforms.Compose([\n",
                "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomVerticalFlip(),\n",
                "    transforms.RandomRotation(20),\n",
                "    transforms.RandomAffine(0, scale=(0.85, 1.15)),\n",
                "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
                "])\n",
                "\n",
                "val_transform = transforms.Compose([\n",
                "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
                "])\n",
                "\n",
                "# ‚îÄ‚îÄ Load datasets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "train_dataset = datasets.ImageFolder(str(TRAIN_DIR), transform=train_transform)\n",
                "val_dataset   = datasets.ImageFolder(str(VAL_DIR),   transform=val_transform)\n",
                "\n",
                "CLASS_NAMES = train_dataset.classes\n",
                "NUM_CLASSES = len(CLASS_NAMES)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
                "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
                "\n",
                "print(f'\\nüìä Dataset:')\n",
                "print(f'   Train:      {len(train_dataset)} images ({len(train_loader)} batches)')\n",
                "print(f'   Validation: {len(val_dataset)} images ({len(val_loader)} batches)')\n",
                "print(f'   Classes:    {NUM_CLASSES}')\n",
                "print(f'   Image size: {IMG_SIZE}x{IMG_SIZE}')\n",
                "print(f'   Pin memory: {PIN_MEMORY}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚îÄ‚îÄ Visualise sample images ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "def denormalize(tensor):\n",
                "    mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
                "    std  = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
                "    return (tensor * std + mean).clamp(0, 1)\n",
                "\n",
                "plt.figure(figsize=(16, 10))\n",
                "images, labels = next(iter(train_loader))\n",
                "for i in range(min(15, len(images))):\n",
                "    plt.subplot(3, 5, i + 1)\n",
                "    img = denormalize(images[i]).permute(1, 2, 0).numpy()\n",
                "    plt.imshow(img)\n",
                "    plt.title(CLASS_NAMES[labels[i]], fontsize=7)\n",
                "    plt.axis('off')\n",
                "plt.suptitle('Sample Training Images', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Build Model ‚Äî MobileNetV2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚îÄ‚îÄ Pretrained MobileNetV2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
                "\n",
                "# Freeze all base layers\n",
                "for param in model.features.parameters():\n",
                "    param.requires_grad = False\n",
                "\n",
                "# Replace classifier head for our 42 classes\n",
                "model.classifier = nn.Sequential(\n",
                "    nn.Dropout(0.3),\n",
                "    nn.Linear(model.last_channel, 256),\n",
                "    nn.ReLU(),\n",
                "    nn.Dropout(0.2),\n",
                "    nn.Linear(256, NUM_CLASSES),\n",
                ")\n",
                "\n",
                "model = model.to(device)\n",
                "\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f'Total params:     {total_params:,}')\n",
                "print(f'Trainable params: {trainable_params:,}')\n",
                "print(f'Frozen params:    {total_params - trainable_params:,}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚îÄ‚îÄ Training function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=5):\n",
                "    best_acc = 0.0\n",
                "    best_model_wts = copy.deepcopy(model.state_dict())\n",
                "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
                "    epochs_no_improve = 0\n",
                "    \n",
                "    for epoch in range(num_epochs):\n",
                "        start = time.time()\n",
                "        \n",
                "        # ‚îÄ‚îÄ Train ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "        model.train()\n",
                "        running_loss, running_correct, running_total = 0.0, 0, 0\n",
                "        \n",
                "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
                "            images = images.to(device, non_blocking=NON_BLOCKING)\n",
                "            labels = labels.to(device, non_blocking=NON_BLOCKING)\n",
                "            \n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            running_loss += loss.item() * images.size(0)\n",
                "            _, preds = torch.max(outputs, 1)\n",
                "            running_correct += (preds == labels).sum().item()\n",
                "            running_total += images.size(0)\n",
                "            \n",
                "            # Progress every 50 batches\n",
                "            if (batch_idx + 1) % 50 == 0:\n",
                "                print(f'  Batch {batch_idx+1}/{len(train_loader)}  '\n",
                "                      f'loss: {running_loss/running_total:.4f}  '\n",
                "                      f'acc: {running_correct/running_total:.4f}', end='\\r')\n",
                "        \n",
                "        train_loss = running_loss / running_total\n",
                "        train_acc  = running_correct / running_total\n",
                "        \n",
                "        # ‚îÄ‚îÄ Validate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "        model.eval()\n",
                "        val_loss_sum, val_correct, val_total = 0.0, 0, 0\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for images, labels in val_loader:\n",
                "                images = images.to(device, non_blocking=NON_BLOCKING)\n",
                "                labels = labels.to(device, non_blocking=NON_BLOCKING)\n",
                "                outputs = model(images)\n",
                "                loss = criterion(outputs, labels)\n",
                "                \n",
                "                val_loss_sum += loss.item() * images.size(0)\n",
                "                _, preds = torch.max(outputs, 1)\n",
                "                val_correct += (preds == labels).sum().item()\n",
                "                val_total += images.size(0)\n",
                "        \n",
                "        val_loss = val_loss_sum / val_total\n",
                "        val_acc  = val_correct / val_total\n",
                "        \n",
                "        scheduler.step(val_loss)\n",
                "        elapsed = time.time() - start\n",
                "        \n",
                "        history['train_loss'].append(train_loss)\n",
                "        history['train_acc'].append(train_acc)\n",
                "        history['val_loss'].append(val_loss)\n",
                "        history['val_acc'].append(val_acc)\n",
                "        \n",
                "        lr = optimizer.param_groups[0]['lr']\n",
                "        print(f'Epoch {epoch+1:2d}/{num_epochs}  '\n",
                "              f'train_loss: {train_loss:.4f}  train_acc: {train_acc:.4f}  '\n",
                "              f'val_loss: {val_loss:.4f}  val_acc: {val_acc:.4f}  '\n",
                "              f'lr: {lr:.1e}  [{elapsed:.0f}s]')\n",
                "        \n",
                "        if val_acc > best_acc:\n",
                "            best_acc = val_acc\n",
                "            best_model_wts = copy.deepcopy(model.state_dict())\n",
                "            torch.save(model.state_dict(), 'best_model.pth')\n",
                "            print(f'  ‚úÖ New best! (val_acc: {val_acc:.4f})')\n",
                "            epochs_no_improve = 0\n",
                "        else:\n",
                "            epochs_no_improve += 1\n",
                "            if epochs_no_improve >= patience:\n",
                "                print(f'  ‚èπ Early stopping ({patience} epochs no improvement)')\n",
                "                break\n",
                "    \n",
                "    model.load_state_dict(best_model_wts)\n",
                "    print(f'\\nüèÜ Best val accuracy: {best_acc:.4f}')\n",
                "    return model, history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚îÄ‚îÄ Phase 1: Train classifier head (base frozen) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "print('üèãÔ∏è Phase 1: Training classifier head (base frozen)...\\n')\n",
                "\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-3)\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
                "\n",
                "model, history1 = train_model(\n",
                "    model, train_loader, val_loader,\n",
                "    criterion, optimizer, scheduler,\n",
                "    num_epochs=15, patience=5\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚îÄ‚îÄ Phase 2: Unfreeze top layers and fine-tune ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "print('\\nüèãÔ∏è Phase 2: Fine-tuning top layers...\\n')\n",
                "\n",
                "# Unfreeze last 5 feature blocks\n",
                "for i, block in enumerate(model.features):\n",
                "    if i >= 14:\n",
                "        for param in block.parameters():\n",
                "            param.requires_grad = True\n",
                "\n",
                "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f'Trainable params: {trainable:,}\\n')\n",
                "\n",
                "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
                "scheduler_ft = optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, patience=2, factor=0.5)\n",
                "\n",
                "model, history2 = train_model(\n",
                "    model, train_loader, val_loader,\n",
                "    criterion, optimizer_ft, scheduler_ft,\n",
                "    num_epochs=15, patience=5\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Training History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "history = {k: history1[k] + history2[k] for k in history1}\n",
                "phase1_end = len(history1['train_acc']) - 1\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "axes[0].plot(history['train_acc'], label='Train', linewidth=2)\n",
                "axes[0].plot(history['val_acc'], label='Validation', linewidth=2)\n",
                "axes[0].axvline(x=phase1_end, color='gray', linestyle='--', alpha=0.7, label='Fine-tune start')\n",
                "axes[0].set_title('Accuracy', fontweight='bold')\n",
                "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Accuracy')\n",
                "axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].plot(history['train_loss'], label='Train', linewidth=2)\n",
                "axes[1].plot(history['val_loss'], label='Validation', linewidth=2)\n",
                "axes[1].axvline(x=phase1_end, color='gray', linestyle='--', alpha=0.7, label='Fine-tune start')\n",
                "axes[1].set_title('Loss', fontweight='bold')\n",
                "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Loss')\n",
                "axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.suptitle('CropGuard ‚Äî Training History', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Evaluate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.eval()\n",
                "y_true, y_pred = [], []\n",
                "\n",
                "with torch.no_grad():\n",
                "    correct, total, loss_sum = 0, 0, 0.0\n",
                "    for images, labels in val_loader:\n",
                "        images = images.to(device, non_blocking=NON_BLOCKING)\n",
                "        labels = labels.to(device, non_blocking=NON_BLOCKING)\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        \n",
                "        loss_sum += loss.item() * images.size(0)\n",
                "        _, preds = torch.max(outputs, 1)\n",
                "        correct += (preds == labels).sum().item()\n",
                "        total += images.size(0)\n",
                "        \n",
                "        y_true.extend(labels.cpu().numpy())\n",
                "        y_pred.extend(preds.cpu().numpy())\n",
                "\n",
                "test_acc  = correct / total\n",
                "test_loss = loss_sum / total\n",
                "\n",
                "print(f'üéØ Validation Accuracy: {test_acc:.4f} ({test_acc*100:.1f}%)')\n",
                "print(f'üìâ Validation Loss:     {test_loss:.4f}')\n",
                "\n",
                "print('\\nüìä Classification Report')\n",
                "print('=' * 70)\n",
                "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚îÄ‚îÄ Confusion Matrix ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "\n",
                "plt.figure(figsize=(20, 16))\n",
                "sns.heatmap(\n",
                "    cm, annot=True, fmt='d', cmap='Greens',\n",
                "    xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
                "    linewidths=0.5, linecolor='white',\n",
                ")\n",
                "plt.title('Confusion Matrix ‚Äî CropGuard', fontweight='bold', fontsize=14)\n",
                "plt.xlabel('Predicted', fontweight='bold')\n",
                "plt.ylabel('Actual', fontweight='bold')\n",
                "plt.xticks(rotation=45, ha='right', fontsize=6)\n",
                "plt.yticks(rotation=0, fontsize=6)\n",
                "plt.tight_layout()\n",
                "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Export for FastAPI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "\n",
                "# ‚îÄ‚îÄ Export directory ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "EXPORT_DIR = Path('./export')\n",
                "EXPORT_DIR.mkdir(exist_ok=True)\n",
                "\n",
                "# Save as TorchScript\n",
                "model.eval()\n",
                "model_cpu = model.cpu()\n",
                "scripted = torch.jit.script(model_cpu)\n",
                "model_path = EXPORT_DIR / 'crop_disease_model.pt'\n",
                "scripted.save(str(model_path))\n",
                "print(f'‚úÖ Model saved: {model_path} ({model_path.stat().st_size / 1024 / 1024:.1f} MB)')\n",
                "\n",
                "# Save class map\n",
                "class_map = {str(i): name for i, name in enumerate(CLASS_NAMES)}\n",
                "class_map_path = EXPORT_DIR / 'class_map.json'\n",
                "with open(class_map_path, 'w') as f:\n",
                "    json.dump(class_map, f, indent=2)\n",
                "print(f'‚úÖ Class map saved: {class_map_path}')\n",
                "\n",
                "# ‚îÄ‚îÄ Copy to Server/model/ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "server_model_dir = Path(r'U:\\CropGuard AI\\Server\\model')\n",
                "server_model_dir.mkdir(exist_ok=True)\n",
                "\n",
                "shutil.copy(model_path, server_model_dir / 'crop_disease_model.pt')\n",
                "shutil.copy(class_map_path, server_model_dir / 'class_map.json')\n",
                "\n",
                "print(f'\\nüì¶ Copied to {server_model_dir}/')\n",
                "print(f'   ‚îú‚îÄ‚îÄ crop_disease_model.pt')\n",
                "print(f'   ‚îî‚îÄ‚îÄ class_map.json')\n",
                "\n",
                "print(f'\\nüìã Class mapping ({NUM_CLASSES} classes):')\n",
                "for idx, name in class_map.items():\n",
                "    print(f'   {idx:>2s} ‚Üí {name}')\n",
                "\n",
                "print(f'\\nüöÄ Restart FastAPI server ‚Üí real predictions!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('‚ïê' * 50)\n",
                "print('  CROPGUARD AI ‚Äî TRAINING SUMMARY')\n",
                "print('‚ïê' * 50)\n",
                "print(f'  Framework:  PyTorch {torch.__version__}')\n",
                "print(f'  Model:      MobileNetV2 (transfer learning)')\n",
                "print(f'  Dataset:    U:\\\\dataset (20K+ images)')\n",
                "print(f'  Classes:    {NUM_CLASSES}')\n",
                "print(f'  Image size: {IMG_SIZE}x{IMG_SIZE}')\n",
                "print(f'  Val Acc:    {test_acc:.4f} ({test_acc*100:.1f}%)')\n",
                "print(f'  Val Loss:   {test_loss:.4f}')\n",
                "print(f'  Device:     {device}')\n",
                "print('‚ïê' * 50)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv (3.14.3)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
